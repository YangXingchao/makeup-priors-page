<!DOCTYPE html>
<html>
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-Z4D04R74NW"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-Z4D04R74NW');
  </script>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Makeup Extraction of 3D Representation via Illumination-Aware Image Decomposition</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Makeup Prior Models for 3D Facial Makeup Estimation and Applications</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://yangxingchao.github.io/publications/" target="_blank">Xingchao Yang</a><sup>1,2</sup>,</span>
                <span class="author-block">
                  <a href="https://taketomitakafumi.sakura.ne.jp/web/en/" target="_blank">Takafumi Taketomi</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://www.cgg.cs.tsukuba.ac.jp/~endo/index_en.html" target="_blank">Yuki Endo</a><sup>2</sup>
                    <span class="author-block">
                      <a href="http://kanamori.cs.tsukuba.ac.jp/index.html" target="_blank">Yoshihiro Kanamori</a><sup>2</sup>
                    </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup> CyberAgent, AI Lab &nbsp;<sup>2</sup> University of Tsukuba <br>CVPR 2024</span>
                    <span class="eql-cntrb"><small><br>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://yangxingchao.github.io/makeup-priors-page/" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/EG2023_MakeupExtraction_Supplementary.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YangXingchao/makeup-priors" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://github.com/YangXingchao/makeup-priors" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/teaser.jpg" alt="Banner image" height="100%">
      <h2 class="subtitle has-text-centered">
        Example of 3D facial makeup estimation and applications using makeup prior models. 
        Top left: The effectiveness of our prior models (PCA and StyleGAN2) for estimating 3D facial makeup layers. 
        Bottom left: The result of 3D face reconstruction using makeup prior models. 
        Our method accurately recovers the makeup of 3D faces and it can be compatible with the existing 3D face reconstruction framework. 
        Right: 3D makeup interpolation and transfer applications using the PCA-based prior model. 
        Note that the StyleGAN2-based prior model has equivalent functionality.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->



<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/eg_makeup_2023.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered"><br>
        3D Avatar is widely used in movies and advertisements, with facial makeup being a crucial aspect of creating these lifelike digital characters. However, the process of editing makeup on a 3D face or using specialized equipment can be time-consuming. To obtain facial makeup for 3D characters, we propose a method to extract makeup from a single face image in a UV format that can be used for 3D face models. We unwarp the input image to UV texture so that it can be applied to a 3D face model. Then, we decompose the UV texture to bare skin, makeup, and lighting effects.
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In this work, we introduce two types of makeup prior models to extend existing 3D face prior models: PCA-based and StyleGAN2-based priors. The PCA-based prior model is a linear model that is easy to construct and is computationally efficient. However, it retains only low-frequency information. Conversely, the StyleGAN2-based model can represent high-frequency information with relatively higher computational cost than the PCA-based model. Although there is a trade-off between the two models, both are applicable to 3D facial makeup estimation and related applications. By leveraging makeup prior models and designing a makeup consistency module, we effectively address the challenges that previous methods faced in robustly estimating makeup, particularly in the context of handling self-occluded faces. In experiments, we demonstrate that our approach reduces computational costs by several orders of magnitude, achieving speeds up to 180 times faster. In addition, by improving the accuracy of the estimated makeup, we confirm that our methods are highly advantageous for various 3D facial makeup applications such as 3D makeup face reconstruction, user-friendly makeup editing, makeup transfer, and interpolation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/recons_1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered"><br>
          Comparison with 3D face reconstruction methods using our makeup prior models. Our methods successfully reconstruct facial makeup. Specifically, our PCA model is capable of broadly recovering makeup colors, while our StyleGAN2 model achieves precise replication of complex makeup features, such as blush and gradational eyeshadow.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/recons_2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered"><br>
          Comparison with 3D face reconstruction methods using our makeup prior models. Our methods successfully reconstruct facial makeup. Specifically, our PCA model is capable of broadly recovering makeup colors, while our StyleGAN2 model achieves precise replication of complex makeup features, such as blush and gradational eyeshadow.
       </h2>
     </div>
     <div class="item">
        <!-- Your image here -->
        <img src="static/images/estimation_1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered"><br>
          Comparison to previous work. Our methods (PCA and StyleGAN2) outperform both Yang-Ext and Yang-Res, which show limitations in handling self-occluded faces.
       </h2>
     </div>
     <div class="item">      
      <!-- Your image here -->
      <img src="static/images/estimation_2.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered"><br>
        Comparison to previous work. Our methods (PCA and StyleGAN2) outperform both Yang-Ext and Yang-Res, which show limitations in handling self-occluded faces.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/method.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered"><br>
          Overview of the makeup estimation network architecture. 
          The network is composed of three modules: The Reconstruction module is pre-trained for 3D face reconstruction; 
          The makeup estimation module infer the makeup coefficient and generates associated makeup textures; 
          The makeup consistency module enhances the effectiveness of makeup estimation.
        </h2>
      </div>
      <div class="item">   
        <!-- Your image here -->
        <img src="static/images/interpolate_pca.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered"><br>
          Results of 3D makeup interpolation and transfer using our PCA model. Left: the bilinear makeup interpolation between four makeup styles. Right: the estimated makeup layers and the interpolated results using the makeup coefficient
        </h2>
      </div>
      <div class="item">   
        <!-- Your image here -->
        <img src="static/images/interpolate_stylegan.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered"><br>
          Results of 3D makeup interpolation and transfer using our StyleGAN2 model. Left: the bilinear makeup interpolation between four makeup styles. Right: the estimated makeup layers and the interpolated results using the makeup coefficient
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->



<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @inproceedings{yang2024makeuppriors,
          author = {Yang, Xingchao and Taketomi, Takafumi and endo, yuki and Kanamori, Yoshihiro},
          title={Makeup Prior Models for {3D} Facial Makeup Estimation and Applications},
          booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
          year={2024}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
